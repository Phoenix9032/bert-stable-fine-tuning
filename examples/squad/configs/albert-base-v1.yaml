input:
  data_dir: /datasets/squad
  train_file: train-v1.1.json
  predict_file: dev-v1.1.json
  version_2_with_negative: false
  overwrite_cache: false # Overwrite the cached training and evaluation sets
  lang_id: 0 # language id of input for language-specific xlm models
  doc_stride: 128 # When splitting up a long document into chunks, how much stride to take between chunks.
  threads: 1 # multiple threads for converting example to features

output:
  log_dir: /logfiles/squad11
  checkpoint_dir: /checkpoints/squad11
  verbose_logging: false # If true, all of the warnings related to data processing will be printed.

tensorboard:
  enable: false
  log_dir: /tb-logs/squad11
  log_histograms: false # Log histograms in tensorboard. They can be quite large (file size) and slow down training. So one might want to disable it

wandb:
  enable: true
  project_name: squad-fine-tuning
  log_dir: /wandb-logs/squad11

model:
  model_type: albert
  model_name_or_path: albert-base-v1 # Path to pre-trained model or shortcut name of huggingface transformer models
  config_name: albert-base-v1 # Pretrained config name or path if not the same as model_name
  tokenizer_name: albert-base-v1 # Pretrained tokenizer name or path if not the same as model_name
  cache_dir: /pre-trained-transformers
  do_lower_case: true # ALBERT vocab is uncased
  max_seq_length: 384
  max_query_length: 64 # The maximum number of tokens for the question
  max_answer_length: 30 # The maximum length of an answer that can be generated.
  null_score_diff_threshold: 0.0
  n_best_size: 5 # The total number of n-best predictions to generate in the nbest_predictions.json output file.

training:
  num_train_epochs: 3
  max_steps: -1 # overrides num_train_epochs
  evaluate_during_training: true
  per_gpu_train_batch_size: 8
  gradient_accumulation_steps: 1
  train_logging_steps: 300 # adjust based on batch_size and number of GPUs used
  eval_logging_steps: 2777 # adjust based on batch_size and number of GPUs used
  save_steps: -1

optimizer:
  learning_rate: 0.00003 # learning-rate should not be too large
  learning_rate_schedule: warmup-linear
  warmup_steps: 0
  weight_decay: 0.0
  adam_epsilon: 0.00000001
  max_grad_norm: 1.0
  fp16: false
  fp16_opt_level: "01" # If fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html

eval:
  eval_all_checkpoints: false
  per_gpu_eval_batch_size: 100 # eval will always run on a single GPU
